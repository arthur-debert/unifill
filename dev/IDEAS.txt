This is what remains to be done on  unifil


[ ] Grep backend

    The idea is to give up some of the ranking sophistication and gain less
    code and much faster speeds, by replacing the custom lua code with
    telescope's builtin grep and fuzzy finding.

    If the results seem promising (that is we get a nice result that's very
    fast) we may consider using a two pass algo to fine tune raning a sumall
    subset of items.

[ ] Verify aliases

    While we are downloading the alias, I've yet to see it show up on  any
    result item, so we should check that we're matching the codepoint dataset
    to the alias one correctly.


[ ] Improve theming

    We're skinning the telescope UI, but we're doing it in a blind way, for
    example by settting text to black regardless of a user's theme.
    The right way to do this is more like use entities (like emphasys text) or
    light background that will be configured on the user's theme, so this can
    be integratted and work regardless of user theming.


[ ] Verify plugin setup
    We did a quick throwaway thing for the setup, now it's time  to make sure
    that the plugin is ready for distribution: 
        - is the file layout looking good? 
        - do we follow plugin desing best practices? If not, what's to be
          changed? 
        - what about the entire dataset generation thing? this shouldnt be
          redistributed. also, should we have the dataset be in git or the
          plugin load it from a zip at load time 


[]  Install from github

    When htat is all done, we can ditch the local install and test the regular
    github based install


[ ] Frequently used
    Currently we're showing / ranking lexographically all characters. IRL  ,
    people will use a subset of these much  more often. 
    We could generate a frequently used rankiong and reserve the top 5 places
    for these, both at cold start (no search query term is entered) and when
        sorting results (that is, if a frequently used one is part of the
        results, it should be bumped up)


[ ] All Unicode, really? 

    Currently we process all unicode chars, in their full glory. But do we need
    to?  The most obvious use case if people wanting a nice arrow, greek
    symbol, stuff like that. 
    We could generate two data sets: classic and unabridged. Default to classic
    but could be changed to unabridged.
    

[ ] Unicodize
    
    Instead of typing / inserting all unicoide, have a unicodize this command
    that searchs the buffer for good canditates and in a search and replace
    sequence like thing proposes changes, like changing PI to ፲ , or -> to →. 

    Is there a good mapping of these? Of course we could let users add their
    own via config .
