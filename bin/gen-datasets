#!/usr/bin/env zsh
# shellcheck shell=bash
#
# gen-datasets - Unicode dataset downloader for the unifill plugin
#
# This script downloads pre-generated Unicode datasets from GitHub releases
# for use with the unifill plugin. By default, it only downloads files that 
# don't already exist in the data directory, making it efficient for 
# incremental updates.
#
# When the --force flag is used, it removes all existing files and downloads them again,
# which is useful for ensuring all data is up-to-date.
#
# The downloaded files are placed in the project's data directory and also copied to
# the XDG_DATA_HOME directory for system-wide access.

# Exit immediately if a command exits with a non-zero status.
set -e

# Get the absolute path of the directory where the script is located
SCRIPT_DIR="${0:a:h}"

# Project root is assumed to be one level up from the 'bin' directory
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

# Use XDG_DATA_HOME if available, otherwise use default location
if [ -n "$XDG_DATA_HOME" ]; then
  XDG_DATA_DIR="$XDG_DATA_HOME/unifill"
else
  XDG_DATA_DIR="$HOME/.local/share/unifill"
fi

# Default destination is the project data directory
# But we'll also copy to XDG_DATA_HOME for system-wide access
DESTINATION_DIR="$PROJECT_ROOT/data"

# GitHub release URL base
GITHUB_RELEASE_URL="https://github.com/arthur-debert/glyph-catcher/releases/latest/download"

# Default dataset to use
DATASET="every-day"

# Force flag to re-download all files
FORCE=false

# Parse command line arguments
while [[ $# -gt 0 ]]; do
  case $1 in
  --force)
    FORCE=true
    shift
    ;;
  --dataset)
    DATASET="$2"
    shift 2
    ;;
  --help | -h)
    cat <<EOF
Usage: $(basename "$0") [OPTIONS]

Download Unicode datasets for the unifill plugin from GitHub releases.

OPTIONS:
  --force          Remove all existing files and download them again
                   Without this flag, only missing files are downloaded
 
  --dataset SET    Specify which dataset to use
                   SET can be: every-day, complete
                   Default: every-day
 
  --help, -h       Display this help message and exit

BEHAVIOR:
  By default, this script only downloads files that don't already exist in the
  data directory. This makes it efficient for incremental updates.

  When --force is used, all existing files are removed before downloading new ones.
  This ensures all data is up-to-date.

  Downloaded files are placed in:
  - Project data directory: $PROJECT_ROOT/data/
  - System-wide directory: \$XDG_DATA_HOME/unifill/ or ~/.local/share/unifill/

EXAMPLES:
  # Download only missing files
  $(basename "$0")

  # Force re-download of all files
  $(basename "$0") --force

  # Download the complete dataset
  $(basename "$0") --dataset complete
EOF
    exit 0
    ;;
  *)
    echo "Unknown option: $1"
    echo "Run '$(basename "$0") --help' for usage information."
    exit 1
    ;;
  esac
done

# Ensure the destination directories exist
mkdir -p "$DESTINATION_DIR"
mkdir -p "$XDG_DATA_DIR"

# If force flag is set, remove all existing files
if [ "$FORCE" = true ]; then
  echo "Force flag set. Removing all existing files in $DESTINATION_DIR..."
  # Create the destination directory if it doesn't exist
  mkdir -p "$DESTINATION_DIR"
  # Use find to remove files safely (avoids "no matches found" error)
  find "$DESTINATION_DIR" -type f -delete
  echo "All files removed."
fi

# Function to check if a file needs to be downloaded
needs_download() {
  local file_name="unicode.$DATASET.lua.gz"
  local file_path="$DESTINATION_DIR/$file_name"

  if [ ! -f "$file_path" ]; then
    return 0 # File doesn't exist, needs download
  elif [ "$FORCE" = true ]; then
    return 0 # Force flag is set, re-download regardless
  else
    return 1 # File exists and no force flag, skip download
  fi
}

# Function to download a file
download_file() {
  local file_name="unicode.$DATASET.lua.gz"
  local url="$GITHUB_RELEASE_URL/$file_name"
  local output_path="$DESTINATION_DIR/$file_name"
  
  echo "Downloading dataset for $DATASET..."
  
  # Use curl to download the file
  if curl -L --fail --silent --show-error --output "$output_path" "$url"; then
    echo "Download successful: $output_path"
    return 0
  else
    echo "Error: Failed to download $url"
    return 1
  fi
}

echo "Unicode dataset downloader for unifill"
echo "------------------------------------"
echo "Dataset to use: $DATASET"
echo "Force re-download: $([ "$FORCE" = true ] && echo "Yes" || echo "No (only downloading missing files)")"
echo ""

# Track if any files were downloaded
FILES_DOWNLOADED=false

# Check if we need to download the file
if needs_download; then
  echo "Downloading lua format..."
  FILES_DOWNLOADED=true
  
  # Download the file
  if download_file; then
    # Copy to XDG data directory for system-wide access
    XDG_FILE_PATH="$XDG_DATA_DIR/unicode.$DATASET.lua.gz"
    cp "$DESTINATION_DIR/unicode.$DATASET.lua.gz" "$XDG_FILE_PATH"
    
    printf "Downloaded: \e[32m%s\e[0m\n" "$DESTINATION_DIR/unicode.$DATASET.lua.gz"
    printf "Copied to: \e[32m%s\e[0m\n" "$XDG_FILE_PATH"
  else
    echo "Error: Failed to download dataset."
    exit 1
  fi
else
  printf "Skipping download: File already exists\n"
fi

echo ""
echo ""
echo "------------------------------------"
echo "SUMMARY"
echo "------------------------------------"
if [ "$FILES_DOWNLOADED" = true ]; then
  echo "✓ Dataset download completed successfully"
  echo ""
  echo "Files are available in:"
  echo "  - Project directory: $DESTINATION_DIR/"
  echo "  - System directory:  $XDG_DATA_DIR/"
  echo ""
  echo "These files are used by the unifill plugin for Unicode character lookup."
else
  echo "✓ No files needed to be downloaded"
  echo "  All requested files already exist in $DESTINATION_DIR/"
  echo ""
  echo "To re-download all files:"
  echo "  $0 --force"
fi
